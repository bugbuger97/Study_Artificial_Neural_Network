{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 소프트맥스 회귀 보충\n",
        "- 소프트맥스 회귀에서 텐서 shape을 중간중간 확인해보는 코드 추가하여 실행해보자.\n",
        "\n",
        "\n",
        "- DataLoader\n",
        "  - 데이터셋을 쉽게 로드하고 미니배치로 데이터를 나누어 모델 학습에 사용할 수 있도록 돕는 중요한 도구이다.\n",
        "  - 주로 대규모 데이터셋을 처리할 때, 효율적으로 데이터를 불러오는 역할을 하며, 모델 학습이나 평가 중에 데이터를 잘 관리하여 처리 속도를 최적화할 수 있도록 한다.\n",
        "- DataLoader의 역할\n",
        "  - 1. 미니배치 생성: 데이터셋을 작은 배치 단위로 나누어 학습에 사용할 수 있도록 해준다.\n",
        "  - 2. Shuffling: 학습 시 데이터셋의 순서를 무작위로 섞어서 모델이 데이터에 대한 순서 의존성을 학습하지 않도록 도와줌.\n",
        "  - 3. 병렬 처리(Parallelism): 멀티스레드를 사용하여 데이터를 병렬로 로드할 수 있어 데이터 입출력 속도를 높여준다.\n",
        "  - 4, 커스터마이징: 데이터 증강, 정규화 등과 같은 데이터를 병렬로 로드할 수 있어 데이터 입출력 속도를 높여준다.\n",
        "  - 즉, DataLoader는 대규모 데이터셋의 효율적인 관리를 통해 학습 프로세스를 간호화하고 최적화는데 중요한 역할을 한다.\n",
        "\n",
        "\n",
        "   \n",
        "    print(\"W: \", list(model.parameters())[0].shape, \"B: \", list(model.parameters()).shape)\n",
        "    # training_epochs = 15\n",
        "    for epoch in range(training_epochs):\n",
        "      avg_cost = 0\n",
        "      total_batch = len(data_loader)\n",
        "\n",
        "      for idx, (x_train, y_train) in enumerate(data_loader):\n",
        "        # 배치크가가 100이므로 아래의 연산에서 x_train은 (100, 784)의 텐서가 된다.\n",
        "        print(\"Original: \", x_train.shape)\n",
        "        x_train = x_train.view(-1, 28*28)\n",
        "        print(\"Reshaped: \", x_train.shape)\n",
        "        # 레이블은 one-hot encoding이 된 상태가 아니라 0 ~ 9의 정수이다.\n",
        "        \n",
        "        prediction = model(x_train)\n",
        "        print(\"Result: \", prediction.shape)\n",
        "        cost = criterion(prediction, y_train)\n",
        "\n",
        "\n",
        "\n",
        "- 1. torchvision의 컴퓨터 비전 데이터는 (batch_size, channel, height, width) 형태의 shape을 가짐.\n",
        "- 2. 이를 소프트맥스 회귀에서 활용하기 위해 (batch_size, data_size) 형태로 reshape 진행\n",
        "- 3. 이후 X*W^T + B 연산이 linear 객체에서 일어나며, (batch_size, class_num) 형태의 결과값을 도출함.\n",
        "  - W.shape: (10, 784) 형태 텐서\n",
        "  - B: (10) 형태 텐서로써 각 클래스 확률에 해당하는 출력값에 각각 더해진다.\n",
        "\n",
        "- 일반적으로 신경망 모델 설명에서 batch_size에 해당하는 dimension은 표시되어 있지 않음에 유의해야 함.\n",
        "- 실제 학습이 수행될 때는 미니 배치 경사하강법으로 수행하기 때문에, batch_size 만큼의 개수에 해당하는 데이터에 대해 일괄 연산을 수행하며, PyTorch에서는 일반적으로 dim 0이 batch_size에 해당됨.\n",
        "\n",
        "- 신경 세포 및 활동 전위\n",
        "  - 신경 세포(뉴런)은 다수의 뉴런으로부터 정보를 전달받으며, 역치 이상의 자극이 들어오면 전기 신호를 발생시켜 다른 뉴런으로 정보를 전달하게 됨.\n",
        "\n",
        "\n",
        "\n",
        "# 퍼셉트론\n",
        "- 실제 신경세포를 본따서 다수의 입력을 받은 뒤 하나의 결과를 내는 데이터 처리 구조를 퍼셉트론이라고 함.\n",
        "- 입력값 X와 가중치 W의 곱을 합산한 뒤 그 값을 비선형 함수로 변형시켜 출력함.\n",
        "  - 비선형 함수 == 계단 함수(step function)이라고 함.\n",
        "  - 실제 뉴런에서의 역치에 해당하는 임계치를 θ로 표현함.\n",
        "\n",
        "- 퍼셉트론의 입력값을 취합한 후 출력을 만들기 위해 사용되는 함수 == 활성 함수(activation function)\n",
        "  - 일반적으로 비선형 함수가 많이 쓰이지만, 경우에 따라 선형 함수도 활용된다.\n",
        "\n",
        "- 로지스틱 회귀는 활성 함수가 sigmoid인 퍼셉트론이라고 볼 수 있음.\n",
        "- 소프트맥스 회귀는 활성 함수가 softmax라고 볼 수 있음.\n",
        "  - 로지스틱 회귀: H(X) = sigmoid(W*X + B)\n",
        "  - 소프트맥스 회귀: H(X) = softmax(W*X + B)\n",
        "  - 초기 퍼셉트론: H(X) = step(W*X + B)\n",
        "  - 선형 회귀: H(X) = linear(W*X + B)\n",
        "    - 이 때, linear() == \"y = x\"를 의미함.\n",
        "\n",
        "- 퍼셉트론에서 각 데이터가 존재하는 단계를 층(layer)이라고 함.\n",
        "- 퍼셉트론은 최소한 입력, 출력 2개의 층이 존재하며, 이렇게 단 2개의 층(입력층, 출력층)만으로 이루어진 퍼셉트론을 \"단층 퍼셉트론\"이라고 함.\n",
        "\n",
        "\n",
        "# 다층 퍼셉트론(MLP)\n",
        "- 총 3개 이상의 층으로 이루어진 퍼셉트론을 다층 퍼셉트론(Multi-Layer Perceptron, MLP)라고 함.\n",
        "- 입력과 출력을 제외한 나머지 층은 은닉층(hidden layer)라고 하는데, 은닉층이 2개 이상인 MLP는 심층 신경망(deep neural network, DNN)이라고 함.\n",
        "\n",
        "- 여기서는 MLP의 활성 함수는 sigmoid라고 가정함.\n",
        "- z는 활성 함수가 적용되기 전의 값이고, h 및 o는 활성 함수가 적용된 값을 의미한다.\n",
        "- MLP에서 각 층의 출력값은 이전 층의 출력값들로 표현 가능\n",
        "- (ex) 순전파 예시\n",
        "      input = [[x1], [x2]]\n",
        "      hidden layer1 = [[W1, W3], [W2, W4]]\n",
        "      input of hidden layer2 = [[h1 = sigmoid(z1)], [h2 = sigmoid(z2)]]\n",
        "      hidden layer2 = [[W5, W7], [W6, W8]]\n",
        "      result of hidden layer2 = [[z3], [z4]]\n",
        "      output = [[o1 = sigmoid(z3)], [o2 = sigmoid(z4)]]\n",
        "\n",
        "- 다층 퍼셉트론에서는 가중치를 어떻게 학습시켜야 하나?\n",
        "  - 역전파 방법을 통해 가중치를 학습 시킨다.\n",
        "\n",
        "\n",
        "# 역전파\n",
        "  - MLP 및 심층 신경망의 파라미터 학습은 역전파 과정을 통해서 이루어진다.\n",
        "  - 순전파: 데이터에 대한 예측값을 계산할 때는 \"입력층 → 은닉층 → 출력층\" 순으로 계산이 진행된다.\n",
        "  - 역전파: 가중치 학습 때는 출력층부터 가중치 업데이트가 진행되므로 역전파라고 함.\n",
        "  - 미분의 연쇄법칙(chain rule)을 이용해서 출력에 가까운 가중치부터 차례차례 업데이트 해나가게 됨.\n",
        "  - 여기서 loss function을 MSE로 가정한다.\n",
        "  \n",
        "  - 1. 역전파 1단계\n",
        "    - 출력층과 마지막 은닉층 사이에 있는 가중치를 업데이트 한다.\n",
        "    - 미분의 연쇄 법칙(chain rule) 활용\n",
        "      - ∂(Total error) / ∂W = (∂(Total error) / ∂(Output)) * (∂(Output) / ∂Z) * (∂Z / ∂W)\n",
        "    - W += W - lr*(∂(Total error) / ∂W) → 이 식을 통해서 가중치 업데이트 진행\n",
        "\n",
        "  - 2. 역전파 2단계\n",
        "    - 은닉층이 2개라고 가정하고 이것에 대해서 설명하겠다.\n",
        "    - 입력층과 첫번째 은닉층 사이의 가중치를 업데이트 해야 하며, W에 대한 미분값을 계산해야 한다.\n",
        "\n",
        "  - 에러를 계산하는데 쓰인 출력값에 가까운 가중치일수록 먼저 미분값을 계산하기가 편하며, 그 이전에 쓰인 가중치들은 앞서 계산한 미분값들을 바탕으로 차례차례 계산할 수 있다.\n",
        "  - 역전파 과정을 통해 가중치를 업데이트하면, 1 iteration의 학습이 수행됨.\n",
        "  \n",
        "- 활성 함수\n",
        "  - sigmoid의 경우, sigmoid의 끝 부분 기울기의 절대값을 보면, 굉장히 작기 때문에, 경사하강법으로 학습할 때, 가중치의 변화량이 매우 작아진다.\n",
        "  - 0에 가까운 미분값들이 여러 번 곱해지게 되면 역전파를 통한 학습(업데이트) 효과가 거의 없다.\n",
        "    - 이러한 현상을 기울기 소실(vanishing gradient)라고 한다.\n",
        "\n",
        "  - 기울기 소실 문제를 해결할 수 있는 간단한 형태의 활성 함수로써 ReLu함수가 있다.\n",
        "  - ReLu: 0 이상의 구간에서는 선형 함수( y = x )와 같으며, 음수 구간에서는 전부 0의 값을 출력함.\n",
        "    - 직선의 한 군데를 꺾어서 비선형으로 만들었다고 생각하면 됨.\n",
        "  - 수학적으로는 x = 0에서의 미분값이 정의되지 않으나, 실제 구현 상에서는 x = 0에서의 미분값을 0으로 사용하는 경우가 많다.    \n",
        "\n",
        "\n",
        "\n",
        "# 다층 퍼셉트론을 이용한 MNIST 분류\n",
        "- 인공신경망 구현\n",
        "  - MNIST 문제를 MLP를 통해 해결하고자 한다.\n",
        "  \n",
        "\n",
        "\n",
        "    import torch\n",
        "    import torchvision.datasets as dsets\n",
        "    import torchvision,transforms as transforms\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torch.nn as nn\n",
        "    import matplotlib.pyplot as plt\n",
        "    import random\n",
        "\n",
        "    USE_CUDA = torch.cuda.is_available() # GPU가 사용 가능하면 True, 아니면 False를 리턴함.\n",
        "    device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU가 사용 가능하면 사용, 아니면 CPU 사용\n",
        "    print(f'{device}를 사용하여 학습한다.')\n",
        "\n",
        "    # for reproducibility(재현성을 위해서 설정하는 것임.)\n",
        "    random.seed(777)\n",
        "    torch.manual_seed(777)\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.manual_seed_all(777)\n",
        "\n",
        "\n",
        "\n",
        "  - MLP부터는 GPU로 학습을 진행하는 것이 훻씬 빠르다.\n",
        "\n",
        "  - 모델 코드\n",
        "    - nn.Sequential은 다양한 nn.Module들을 직렬로 연결한 리스트라고 볼 수 있다.\n",
        "    - 콤마(,)로 구분하여 원하는 만큼 nn.Module들을 생성자 함수에 입력할 수 있으며, 그러면 입력한 순서대로 forward 시 각 모듈들이 실행된다.\n",
        "\n",
        "\n",
        "\n",
        "    class MLP(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "          nn.Linear(784, 100),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(100, 100),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(100,10))\n",
        "      \n",
        "      def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "  \n",
        "\n",
        "  - 학습 코드\n",
        "    - GPU를 사용할 때는 텐서 및 nn.Module들을 GPU 메모리로 옮겨주어야 함.\n",
        "      - 앞에서 device를 설정하였기 때문에, to(device)를 통해 GPU 변수로 변경가능\n",
        "      - GPU 변수로 옮기고 싶다면, cuda()를 통해서도 가능함.\n",
        "    - Adam은 SGD보다 발전된 형태의 경사하강법의 일종이다. 널리 쓰이지만 가끔 과적합 문제가 발생한다.\n",
        "\n",
        "\n",
        "    # hyperparameters\n",
        "    training_epochs = 3\n",
        "    batch_size = 100\n",
        "\n",
        "    # MNIST dataset\n",
        "    mnist_train = dsets.MNIST(root = 'MNIST_data/', train = True, transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "    mnist_test = dsets.MNIST(root = 'MNIST_data/', train = False, transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "    # dataset loader\n",
        "    data_loader = DataLoader(dataset = mnist_train, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    # MLP 객체 생성\n",
        "    model = MLP().to(device)\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "    # 학습 loop\n",
        "    for epoch in range(training_epochs):\n",
        "      avg_cost = 0\n",
        "      total_batch = len(data_loader)\n",
        "\n",
        "      for idx, (x_train, y_train) in enumerate(data_loader):\n",
        "        x_train = x_train.view(-1, 28*28).to(device)\n",
        "        y_train = y_train.to(device)\n",
        "\n",
        "        outputs = model(x_train)\n",
        "        cost = criterion(outputs, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "      print('Epoch:','%04d' %(epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "    print('Learning finished')\n",
        "\n",
        "\n",
        "\n",
        "  - 테스트 코드 1\n",
        "    - 단 3epoch만 학습시켰음에도 소프트맥스 회귀의 15epoch 학습 후 88%보다 훨씬 높은 성능을 보임.\n",
        "\n",
        "\n",
        "\n",
        "    # 테스트 데이터를 사용하여 모델을 테스트한다.\n",
        "    with torch.no_grad(): # torch.no_grad()를 하면, gradient 계산을 수행하지 않는다.\n",
        "      x_test = mnist_test.data.view(-1, 28*28).float().to(device)\n",
        "      y_test = mnist_test.targets.to(device)\n",
        "\n",
        "      prediction = model(x_test)\n",
        "      correct_prediction = torch.argmax(prediction, 1) == y_test\n",
        "      accuracy = correct_prediction.float().mean()\n",
        "      print('Accuracy:', accuracy.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  - 테스트 코드 2\n",
        "    - 어떤 샘플들이 틀린 것인지 직접 확인해보기\n",
        "      \n",
        "      \n",
        "      \n",
        "    with torch.no_grad():\n",
        "      # MNIST 테스트 데이터에서 틀린 샘플 중 무작위로 하나를 뽑아서 예측을 해본다.\n",
        "      wrong_indices = (correct_prediction == 0).nonzero(as_tuple=True)[0] # 틀린 샘플들 인덱스 추출\n",
        "      r = random.randint(0, len(wrong_indices)-1)\n",
        "      r = wrong_indices[r].cpu().item()\n",
        "      x_single_data = mnist_test.data[r:r+1].view(-1,28*28).float().cuda()\n",
        "      y_single_data = mnist_test.targets[r:r+1].cuda()\n",
        "\n",
        "      print('Label:',y_single_data.item())\n",
        "      single_prediction = model(x_single_data)\n",
        "      print('Prediction:', torch.argmax(single_prediction,1).item())\n",
        "\n",
        "      plt.imshow(mnist_test.data[r:r+1].view(28,28), cmap = 'Greys', interpolation='nearest')\n",
        "      plt.show()\n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "xfn13xdXIlxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "67w3ypGVfxDv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "print(f'GPU가 사용가능 여부: {USE_CUDA}')\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(f'다음 기기로 학습한다: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GfOr5nXfwl4",
        "outputId": "c7bccfb8-4470-4725-a1b8-c793b7f58f9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU가 사용가능 여부: True\n",
            "다음 기기로 학습한다: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "U2-4bSR-fwo9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP,self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(784,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100,10)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "QMv0kYRifwrN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "training_epochs = 3\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "SDyDrnMifwuI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root = 'MNIST_data/', train = True, transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root = 'MNIST_data/', train = False, transform = transforms.ToTensor(), download = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqPVqtnNh65Z",
        "outputId": "b607c076-4506-4ec6-f55b-78bf341fd39e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:11<00:00, 899584.78it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134055.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1284427.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10467323.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset loader\n",
        "data_loader = DataLoader(dataset = mnist_train, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "5nQBWHyhiHW4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP 객체 생성\n",
        "model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "EYn0USClh9Js"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 loop\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "  total_batch = len(data_loader)\n",
        "\n",
        "  for idx, (x_train, y_train) in enumerate(data_loader):\n",
        "    x_train = x_train.view(-1, 28*28).to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    outputs = model(x_train)\n",
        "    cost = criterion(outputs, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += cost / total_batch\n",
        "\n",
        "  print('Epoch:','%04d' %(epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY8J4TVbh9aT",
        "outputId": "91cda419-3fc3-456a-de9a-c93fa567f4ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 0.244781151\n",
            "Epoch: 0002 cost = 0.134417817\n",
            "Epoch: 0003 cost = 0.106764004\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터를 사용하여 모델을 테스트한다.\n",
        "with torch.no_grad(): # torch.no_grad()를 하면, gradient 계산을 수행하지 않는다.\n",
        "  x_test = mnist_test.data.view(-1, 28*28).float().to(device)\n",
        "  y_test = mnist_test.targets.to(device)\n",
        "\n",
        "  prediction = model(x_test)\n",
        "  correct_prediction = torch.argmax(prediction, 1) == y_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gww1jibisDg",
        "outputId": "644f3284-dc6b-4e4e-d45f-a1bdfdfda256"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9627999663352966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # MNIST 테스트 데이터에서 틀린 샘플 중 무작위로 하나를 뽑아서 예측을 해본다.\n",
        "  wrong_indices = (correct_prediction == 0).nonzero(as_tuple=True)[0] # 틀린 샘플들 인덱스 추출\n",
        "  r = random.randint(0, len(wrong_indices)-1)\n",
        "  r = wrong_indices[r].cpu().item()\n",
        "  x_single_data = mnist_test.data[r:r+1].view(-1,28*28).float().cuda()\n",
        "  y_single_data = mnist_test.targets[r:r+1].cuda()\n",
        "\n",
        "  print('Label:',y_single_data.item())\n",
        "  single_prediction = model(x_single_data)\n",
        "  print('Prediction:', torch.argmax(single_prediction,1).item())\n",
        "\n",
        "  plt.imshow(mnist_test.data[r:r+1].view(28,28), cmap = 'Greys', interpolation='nearest')\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "CKx0zXifisG1",
        "outputId": "266a44d6-3be7-458b-a0c9-5ba3f4f50379"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 8\n",
            "Prediction: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 머신러닝 모델의 평가\n",
        "- 훈련 데이터 셋으로 학습 완료된 머신러닝 모델의 평가는 테스트 데이터 셋으로 이루어짐.\n",
        "  - 테스트 데이터는 절대로 학습 시에는 활용되면 안됨. 학습 과정에서 테스트 데이터가 포함되어 있고, 이후에 다시 테스트 데이터로 성능을 평가한다면 컨닝과 같다.\n",
        "\n",
        "- 학습을 언제 마칠 것인지 결정하기 위해, 혹은 학습 도중 모델의 성능을 가늠해보기 위해서 훈련 데이터 셋 중 일부를 따로 떼어 평가용으로 쓰기도 하는데 이를 검증 데이터 셋이라고 함.\n",
        "\n",
        "- 전체 데이터(Original Data)\n",
        "  - 훈련(Training)-테스트(Testing)\n",
        "  - 훈련(Training)-검증(Validation)-테스트(Testing)\n",
        "\n",
        "# 과적합\n",
        "- 머신러닝 모델을 학습시키는 목적: 새로운 데이터(테스트 데이터 혹은 아예 새로운 추론 대상)에 대해 분류 또는 회귀를 잘 수행하고자 하는 것임.\n",
        "  - 이미 답을 알고 있는 학습 데이터 셋에 대해서만 잘 동작하는 머신러닝 모뎋을 만들거라면, 규칙 기반 프로그램으로 대체할 수 있다.\n",
        "  - 과적합된 모델은 번거롭고 무겁기 때문에 쓸 이유가 없다.\n",
        "\n",
        "- 머신러닝 모델이 학습 데이터 셋에서 좋은 성능을 보인다고 해서 반드시 새로운 데이터에 대해서 좋은 성능을 보이는 것은 아니다.\n",
        "\n",
        "- 과적합이란 무엇인가?\n",
        "  - 전혀 학습이 되지 않은 상태 == 학습이 부족한 상태 ==  과소 적합\n",
        "  - 학습 데이터 셋 내의 측정 오차까지 모델에 반영해버려서 실제 x, y의 상관관계보다 훨씬 복잡한 관계를 표현하게 됨. 이렇게 필요 이상으로 과하게 학습되는 형상을 과적합이라고 한다.\n",
        "\n",
        "- 최신 심층신경망 구조들은 주어진 데이터 셋 내의 상관관계를 학습하는 능력이 탁월하지만, 과적합이 일어나면 테스트 정확도가 대폭 감소하는 문제가 발생함.\n",
        "\n",
        "- 데스트 데이터 셋은 학습 도중에는 활용할 수 없으므로, 검증 데이터 셋을 이용해서 테스트 데이터 셋에서 얼마나 잘 작동할지 가늠해보는 수 밖에 없다.\n",
        "\n",
        "- 테스트 성능이 최대가 되는 이 시점을 학습 도중 어떻게 알 수 있을까?\n",
        "  - 검증 데이터 셋 성능이 최고점인 상태의 가중치 및 편향을 테스트에 활용함.\n",
        "  - 조기 종료(Early Stopping)\n",
        "    - 검증 데이터 셋을 활용한 매우 효과적인 과적합 방지 기법\n",
        "    - 상세 과정\n",
        "      - 1. 매 epoch마다 검증 데이터 셋에서의 성능을 측정\n",
        "      - 2. 검증 데이터 셋에서의 성능이 최고치를 찍고 점점 안좋아지기 시작하면, 미리 정해둔 epoch 수 만큼은 기다림(예를 들어 5 epoch)\n",
        "      - 3. 만약 미리 정해둔 epoch 수가 다하기 전에 다시 최고 성능이 갱신되면 학습을 계속함.\n",
        "      - 4. 미리 정해둔 epoch 수가 다하도록 최고 성능이 갱신되지 않으면 학습을 종료하고, 검증 데이터 셋에서 가장 높은 성능을 보인 모델 가중치를 이용해 테스트를 진행함.\n",
        "\n",
        "  - 조기 종료(Early Stopping) 예시\n",
        "    - 검증 데이터 셋에서의 성능이 더 안좋아져도 기다리는 epoch 수를 보통 patience하고 지칭함.  \n",
        "    - Patience가 3인 경우 예시\n",
        "      - Epoch 4에서 검증 데이터 셋에서의 정확도가 90.32로 최고 성능을 달성함.\n",
        "      - Eopch 5 ~ 7에서 연속 3번동안 최고 성능을 갱신하지 못하였으므로 patience가 소진되어 학습이 종료됨.\n",
        "      - 최종 모델은 epoch 4에서 저장된 가중치 및 편향 사용\n",
        "  \n",
        "  - 조기 종료(Early Stopping)에서 patience의 필요성\n",
        "    - 실제 딥러닝 학습 과정에서 보이는 loss 혹은 정확도 추이는 다소 불규칙한 모습을 보이므로 최고 성능 달성 후 성능이 떨어졌다고 바로 학습을 종료하면 더 좋은 모델을 얻을 기회를 잃을 수 있음.  \n"
      ],
      "metadata": {
        "id": "U4Q5WypHe1iE"
      }
    }
  ]
}